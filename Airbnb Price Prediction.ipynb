{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a95e1419",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Effects of Hosting Efforts on Airbnb Listing Price\"\n",
    "output:\n",
    "  pdf_document: default\n",
    "  html_document: default\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47d98c8",
   "metadata": {
    "name": "setup",
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "knitr::opts_chunk$set(echo = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6259f44",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Airbnb, a P2P online platform which provides accommodation for traveling guests, has become one of the biggest players in the hospitality industry and a reliable source of\n",
    "income for many others all over the world.\n",
    "\n",
    "From previous studies regarding the effects of different accommodation attributes\n",
    "on rental prices for Airbnb properties, we know that location plays an important role on the price of Airbnb listings. In this study, in addition to location factors, we want to look at other factors which can be controlled by hosts, such as cleanliness of the room, communication with guests, check-in experience, accuracy of listing description, and their potential effects on prices. The factors described are measured by a hostâ€™s review scores^[ average scores for each category is a number from 1(worst) to 5(best)]\n",
    "left by previous guests, which can be seen publicly\n",
    "by potential bookers. We hope to find out whether there is a tendency or ability\n",
    "for hosts to charge higher prices given their efforts in preparation(cleanliness), honesty(accuracy), communication, and hospitality(check-in experience)? We hope to use\n",
    "our findings to help hosts understand the impact of their hosting efforts on their ability to charge higher\n",
    "prices and help them make informed decisions on pricing strategies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ac8761",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "set.seed(1)\n",
    "\n",
    "library(readr)\n",
    "library(dplyr)\n",
    "library(tidyverse)\n",
    "all_listings <- read_csv(\"listings.csv\")\n",
    "head(all_listings, n = 100)\n",
    "\n",
    "dollar_regex <- \"\\\\$\"\n",
    "\n",
    "clean1 <- all_listings%>%\n",
    "  filter(room_type == \"Private room\",\n",
    "         number_of_reviews >= 50,\n",
    "         bedrooms == 1,\n",
    "         )%>%\n",
    "  mutate(price = stringr::str_remove(price, dollar_regex))%>%\n",
    "  mutate_at(\"price\", as.numeric)%>%\n",
    "  dplyr::select(listing_url = listing_url,\n",
    "                property_type = property_type,\n",
    "                price = price,\n",
    "                score_overall = review_scores_rating,\n",
    "                score_value = review_scores_value,\n",
    "                score_cleanliness = review_scores_cleanliness,\n",
    "                score_accuracy = review_scores_accuracy,\n",
    "                score_location = review_scores_location,\n",
    "                score_communication = review_scores_communication,\n",
    "                score_checkin = review_scores_checkin,\n",
    "                accommodates = accommodates,\n",
    "                bedrooms = bedrooms,\n",
    "                beds = beds)\n",
    "\n",
    "train <- clean1[sample(1:nrow(clean1), 430, replace = F),]\n",
    "test <- clean1[which(!(clean1$listing_url %in% train$listing_url)),]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ff1923",
   "metadata": {},
   "source": [
    "# Methods\n",
    "\n",
    "## Variable Selection\n",
    "\n",
    "In context of the research question, the variables we choose are specific to factors related to hosting experience that can be controlled by Airbnb hosts. In addition to location, which is backed by research to have significant positive effect on prices[1], our full model will include:\n",
    "\n",
    "- Price: price per night in Euros.\n",
    "\n",
    "- location score: accessibility and how well the place is located.\n",
    "\n",
    "- cleanliness score: cleanliness of the place.\n",
    "\n",
    "- communication score: How well did the host communicates with their guests.\n",
    "\n",
    "- check-in score: Check-in experience. \n",
    "\n",
    "- accuracy score: Was it accurately represented with up-to-date photos and info?\n",
    "\n",
    "Goal: We are hoping to include all of the above variables in our final model to see which of them have significant effects on price. The primary goal is to construct a model where our T-tests and estimated coefficients are reliable, accurate, and representative of the population. Therefore, the \"best\" model will ideally have good model assumptions and low degree of collinearity(VIF); Goodness of fit(AIC, corrected AIC, BIC, adjusted $R^2$) will be of secondary importance. \n",
    "\n",
    "*In this study, we will only include listings which have reviews of over 50 because we want to look at experienced Airbnb hosts and their pricing strategies, rather than hosts who just started their Airbnb business and are playing around with different pricing strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399faba5",
   "metadata": {},
   "source": [
    "## Model Violations & Diagnostics\n",
    "Starting with the full model, we will perform a EDA to forcast any potential problems. Then we will make predictor\n",
    "vs. predictor plots, $Y vs. \\hat{Y}$ plot to see if condition 1 and 2 hold. Specifically, we are looking for linear\n",
    "relationships between predictors. Points should be spread evenly along the diagonal line in $Y vs. \\hat{Y}$ plot. Then, we will check residual plots to see if linearity assumption, uncorrelated errors assumption, and constant variance assumption hold. Specifically, we are looking for random scattered points around 0. eg. no curves, fanning out pattern, and no large clusters of residuals that have obvious separation from the rest. Lastly, we will check the normality assumption. We are looking for a straight diagonal string of points with minimal deviations at the ends. If any of the conditions or assumptions are not satisfied, we will use box-cox transformation or other variable transformation to try and correct them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df9ca4f",
   "metadata": {},
   "source": [
    "## Problematic Observations\n",
    "\n",
    "We will check for problematic observations: leverage points, outliers, and influential points using measures like $h_{ii}, r_i$, Cook's distance, DFFITS, DFBETAS. We are looking for points whose measures exceed the cutoffs. Those influential points may negatively impact the validity and accuracy of our model predictions and population descriptions. We will look at them and see if there are any reasons to remove them, such as measurement or recording error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0966120e",
   "metadata": {},
   "source": [
    "## Auto Selection Methods\n",
    "\n",
    "1. Starting with the full model after variable transformations, we will use the all possible subsets method to fit the \"best\" models(in terms of adjusted $R^2$) of different number of predictors. We will choose the best 5 models for models of different sizes, eg. top 5 models with two predictors, three predictors, and so on. This way, we get to examine different models with different combinations of predictors without penalizing and eliminating the ones with relatively lower $R^2$ but possibly have better model assumptions and lower VIF's. We will also use auto selection methods for backward, forward, step-wise approaches using measure of AIC and BIC and use them as our potential models.\n",
    "\n",
    "2. With all the possible models, we will start by eliminating the ones with very high collinearity(VIF values) and the ones with bad model assumptions.\n",
    "\n",
    "3. We will then compare AIC, corrected AIC, BIC, adjusted $R^2$ for the models with relatively good model assumptions and low VIF's, and eliminate the ones with high AIC, corrected AIC, BIC values and low adjusted R^2(generally bad fit of model for data).\n",
    "\n",
    "4. Lastly, with the remaining models, we will select our final model based on how well it does overall in terms of degree of collinearity, model assumptions, and goodness of fit using measures previously mentioned.\n",
    "\n",
    "## Model Validation\n",
    "\n",
    "Data will be split randomly to a 1:1 ratio without replacement. We will use the training dataset to perform model building and diagnostics. The test dataset will be used to validate our final model.\n",
    "\n",
    "We will begin by performing a small EDA to check if there are major differences in the two datasets in terms of their means and standard deviations. If the difference is big, then it can potentially be the reason we cannot validate our models.\n",
    "\n",
    "After we have our final model, we will compare it to the same model fit on the test dataset. We are hoping to see similar properties. Specifically, we are hoping to see minimal differences in the estimated coefficients, p-values, same predictors appearing as significant, minimal changes in model assumptions, similar residual sum of squares($RSS$), and adjusted $R^2$. We do not want to see new, or worse model violations. The differences in estimated coefficients should be within two standard errors of coefficients in the training data.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc88c57",
   "metadata": {
    "fig.height": 10,
    "fig.width": 15,
    "lines_to_next_cell": 0,
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "mod <- lm(price ~ score_cleanliness + score_accuracy + score_location + score_communication + score_checkin, data = train)\n",
    "summary(mod)\n",
    "\n",
    "\n",
    "# condition 2\n",
    "pairs(train[6:10])\n",
    "\n",
    "# condition 1\n",
    "plot(train$price ~ fitted(mod), main=\"Y versus Y-hat\", xlab=\"Y-hat\", ylab=\"Y\")\n",
    "abline(a = 0, b = 1)\n",
    "lines(lowess(train$price ~ fitted(mod)), lty=2)\n",
    "\n",
    "# normality\n",
    "r <- rstandard(mod)\n",
    "qqnorm(r)\n",
    "qqline(r)\n",
    "\n",
    "\n",
    "# residual plots\n",
    "par(mfrow=c(2,2))\n",
    "plot(r ~ fitted(mod))\n",
    "plot(r ~ train$score_accuracy, main=\"title\", xlab=\"accuracy\", ylab=\"res\")\n",
    "plot(r ~ train$score_location, main=\"title\", xlab=\"location\", ylab=\"res\")\n",
    "plot(r ~ train$score_cleanliness)\n",
    "plot(r ~ train$score_communication)\n",
    "plot(r ~ train$score_checkin)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c07549",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03be819b",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# all possible models\n",
    "\n",
    "# p <- powerTransform(cbind(train[,3], train[6:10]), family = \"bcnPower\")\n",
    "# summary(p)\n",
    "\n",
    "# car package not working on jupyterhub because the version is old\n",
    "\n",
    "train$t_response <- log(train$price)\n",
    "test$t_response <- log(test$price)\n",
    "train_data <- train[, -c(1:5, 11:13)]\n",
    "test_data <- test[, -c(1:5, 11:13)]\n",
    "head(train_data)\n",
    "head(test_data)\n",
    "\n",
    "library(MASS)\n",
    "## k = 2 is AIC, k = log(n) is BIC\n",
    "##########################################################\n",
    "\n",
    "# AIC\n",
    "## forward selection \n",
    "stepAIC(lm(t_response ~ 1, data = train_data), scope = list(upper=lm(t_response ~ ., data = train_data)), direction = \"forward\", k = 2)\n",
    "## backward selection\n",
    "stepAIC(lm(t_response ~ ., data = train_data), scope = list(lower=lm(t_response ~ 1, data = train_data)), direction = \"backward\", k = 2)\n",
    "## stepwise\n",
    "stepAIC(lm(t_response ~ ., data = train_data), direction = \"both\", k = 2)\n",
    "\n",
    "\n",
    "# same model\n",
    "\n",
    "AIC_best <- lm(formula = t_response ~ score_cleanliness + score_accuracy + \n",
    "    score_location + score_checkin, data = train_data)\n",
    "summary(AIC_best)\n",
    "vif(AIC_best)\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# BIC\n",
    "# forward\n",
    "stepAIC(lm(t_response ~ 1, data = train_data), scope = list(upper=lm(t_response ~ ., data = train_data)), direction = \"forward\", k = log(nrow(train_data)))\n",
    "# backward\n",
    "stepAIC(lm(t_response ~ ., data = train_data), scope = list(upper=lm(t_response ~ 1, data = train_data)), direction = \"backward\", k = log(nrow(train_data)))\n",
    "# stepwise\n",
    "stepAIC(lm(t_response ~ ., data = train_data), direction = \"both\", k = log(nrow(train_data)))\n",
    "\n",
    "# results\n",
    "\n",
    "#forward\n",
    "BIC_forward <- lm(formula = t_response ~ score_location, data = train_data)\n",
    "summary(BIC_forward)\n",
    "\n",
    "#backward\n",
    "BIC_backward <- lm(formula = t_response ~ score_cleanliness + score_accuracy + \n",
    "    score_location, data = train_data)\n",
    "summary(BIC_backward)\n",
    "vif(BIC_backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8081f371",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Results\n",
    "\n",
    "## Description of Data & EDA\n",
    "\n",
    "### Figure 1: Histograms of Response and Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cc418b",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "par(mfrow=c(2,3))\n",
    "hist(train$price, breaks = 20,main = \"Histogram of Prices\", xlab = \"Price\")\n",
    "hist(train$score_cleanliness, breaks = 12,main = \"Histogram of Cleanliness\", xlab = \"Cleanliness Score\")\n",
    "hist(train$score_accuracy, breaks = 12,main = \"Histogram of Accuracy\", xlab = \"Accuracy Score\")\n",
    "hist(train$score_location, breaks = 12,main = \"Histogram of Location\", xlab = \"Location Score\")\n",
    "hist(train$score_communication, breaks = 12,main = \"Histogram of Communication\", xlab = \"Communication Score\")\n",
    "hist(train$score_checkin, breaks = 12,main = \"Histogram of Checkin\", xlab = \"Checkin Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79f7658",
   "metadata": {},
   "source": [
    "Distribution of the response, price, is approximately normal, albeit right skewed, so normality assumption may be violated. There are some observations on the right end which are very different from the rest and could be influential points which may negatively impact the accuracy of our estimated coefficients and other model results. All of the predictors are left-skewed which could lead to non-linearity. Transformations on these variables might also be needed to correct possible violated assumptions. In general, there do not appear to be any significant problems with our variables.\n",
    "\n",
    "\\newpage\n",
    "\n",
    "## Steps and Proccess of Obtaining Final Model\n",
    "\n",
    "Starting with the full model:\n",
    "\n",
    "$price = \\beta_0 + \\beta_1Location + \\beta_2Cleanliness + \\beta_3Accuracy + \\beta_4Communication + \\beta_5Checkin + \\epsilon$\n",
    "\n",
    "### 1. Checking conditions & assumptions\n",
    "\n",
    "For the full model, the two conditions seem to be satisfied. We did not observe weird non-linear relationships between predictors. As well, a transformation on the response variable can be applied to fix condition 1. Normality assumption does not seem to be satisfied as there were significant lift of points in the right tail of our QQ plot. Residual plots will be checked after variable transformations.\n",
    "\n",
    "### 2. Variable transformations\n",
    "\n",
    "We did not use the box-cox transformations outputted by R as they did not improve the conditions and assumptions for our model. Condition 1 improved a lot but the normality assumption did not. There were significant lifts on both ends of the tails in our QQ plot.\n",
    "\n",
    "The transformation that improved our model assumptions is a log transformation to our response. The transformed model is fitted as below. \n",
    "\n",
    "$log(price) = \\beta_0 + \\beta_1Location + \\beta_2Cleanliness + \\beta_3Accuracy + \\beta_4Communication + \\beta_5Checkin + \\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace53ae6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### 3. Checking Conditions & Model Assumptions of Transformed Model\n",
    "\n",
    "### Figure 2: Conditions and Model Assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c24d6",
   "metadata": {
    "fig.height": 5,
    "fig.width": 10,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "train$t_response <- log(train$price)\n",
    "test$t_response <- log(test$price)\n",
    "\n",
    "t_mod <- lm(t_response ~ score_cleanliness + score_accuracy + score_location + score_communication + score_checkin, data = train)\n",
    "\n",
    "par(mfrow=c(1,2))\n",
    "\n",
    "r <- rstandard(t_mod)\n",
    "qqnorm(r)\n",
    "qqline(r)\n",
    "\n",
    "# condition 1\n",
    "plot(log(train$price) ~ fitted(t_mod), main=\"Y versus Y-hat\", xlab=\"Y-hat\", ylab=\"Y\")\n",
    "abline(a = 0, b = 1)\n",
    "lines(lowess(log(train$price) ~ fitted(t_mod)), lty=2)\n",
    "\n",
    "# residual plots\n",
    "par(mfrow=c(2,3))\n",
    "\n",
    "plot(r ~ fitted(t_mod), main=\"Fitted Values vs. Residuals\", xlab=\"accuracy\", ylab=\"res\")\n",
    "plot(r ~ train$score_accuracy, main=\"Accuracy vs. Residuals\", xlab=\"accuracy\", ylab=\"res\")\n",
    "plot(r ~ train$score_location, main=\"Location vs. Residuals\", xlab=\"location\", ylab=\"res\")\n",
    "plot(r ~ train$score_cleanliness, main=\"Cleanliness vs. Residuals\", xlab=\"Cleanliness\", ylab=\"res\")\n",
    "plot(r ~ train$score_communication, main=\"Comm. vs. Residuals\", xlab=\"Communication\", ylab=\"res\")\n",
    "plot(r ~ train$score_checkin, main=\"Checkin vs. Residuals\", xlab=\"Checkin\", ylab=\"res\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22534fe4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Conditions 1 and 2 (see appendix, figure 1) are satisfied: points are spread evenly along the diagonal line in $Y vs. \\hat{Y}$ plot and no observations of weird non-linear relationships between predictors. Residual plots also look good: points are randomly scattered like a cloud around 0. There are some lifts of points on the right end of our QQ plot, but normality assumption is still reasonably satisfied, though not perfect.\n",
    "\n",
    "### 4. Check for leverage points, outliers, and influential points (see appendix section, figure 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a749104",
   "metadata": {
    "fig.height": 10,
    "fig.width": 15,
    "lines_to_next_cell": 2,
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "## leverage points\n",
    "\n",
    "head(train)\n",
    "n <- length(train$price)\n",
    "p <- length(coef(t_mod))-1\n",
    "# calculate the leverage values and compare to cutoff\n",
    "h <- hatvalues(t_mod)\n",
    "hcut <- 2*(p+1)/n\n",
    "# which observations are leverage points?\n",
    "w1 <- which(h > hcut)\n",
    "w1\n",
    "\n",
    "\n",
    "# outliers\n",
    "\n",
    "r_standard <- rstandard(t_mod)\n",
    "# which observations are outliers?\n",
    "w2 <- which(r_standard < -4 | r_standard > 4)\n",
    "w2\n",
    "\n",
    "\n",
    "Dcutoff <- qf(0.5, p+1, n-p-1)\n",
    "D <- cooks.distance(t_mod)\n",
    "which(D > Dcutoff)\n",
    "## named integer(0)\n",
    "# find the DFFITS and compare to cutoff\n",
    "DFFITScut <- 2*sqrt((p+1)/n)\n",
    "dfs <- dffits(t_mod)\n",
    "w3 <- which(abs(dfs) > DFFITScut)\n",
    "w3\n",
    "## 44  66  88 106 124 148 149 163 174 221 223 \n",
    "\n",
    "# find the DFBETAS and compare to cutoff (notice the dimension of DFBETAS)\n",
    "DFBETAcut <- 2/sqrt(n)\n",
    "dfb <- dfbetas(t_mod)\n",
    "\n",
    "w4 <- which(abs(dfb[,1]) > DFBETAcut)\n",
    "w4\n",
    "\n",
    "w5 <- which(abs(dfb[,2]) > DFBETAcut)\n",
    "w5\n",
    "\n",
    "w6 <- which(abs(dfb[,3]) > DFBETAcut)\n",
    "w6\n",
    "\n",
    "w7 <- which(abs(dfb[,4]) > DFBETAcut)\n",
    "w7\n",
    "\n",
    "w8 <- which(abs(dfb[,5]) > DFBETAcut)\n",
    "w8\n",
    "w9 <- which(abs(dfb[,6]) > DFBETAcut)\n",
    "w9\n",
    "w <- unique(c(w3, w4, w5, w6, w7, w8, w9))\n",
    "w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a233c87",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We have 430 observations in our training data set; we observed 47 leverage points, 3 outliers, and 44 unique influential points determined by measures of Cook's distance, DFFITS, and DFBETAS We did not remove any of them because they were not measurement or recording errors.\n",
    "\n",
    "### 5. Using Auto Selection Methods To Fit Possible Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f54c44",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "remove_cell",
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "install.packages(\"leaps\", repos = \"http://cran.us.r-project.org\")\n",
    "library(leaps)\n",
    "\n",
    "train$t_response <- log(train$price)\n",
    "head(train)\n",
    "\n",
    "## all subsets\n",
    "\n",
    "best <- regsubsets(t_response ~ score_cleanliness + score_accuracy + score_checkin + score_communication + score_location, data = train, nbest = 5)\n",
    "#\n",
    "summary(best)\n",
    "\n",
    "# subsets(best, statistic = \"adjr2\")\n",
    "\n",
    "\n",
    "mod1<- lm(t_response ~  score_location, data = train)\n",
    "mod12 <- mod1<- lm(t_response ~  score_accuracy, data = train)\n",
    "mod21 <- lm(t_response ~ score_accuracy + score_location, data = train)\n",
    "mod22 <- lm(t_response ~ score_checkin + score_location, data = train)\n",
    "mod23 <- lm(t_response ~ score_communication + score_location, data = train)\n",
    "mod24 <- lm(t_response ~ score_cleanliness + score_location, data = train)\n",
    "mod31 <- lm(t_response ~ score_accuracy + score_communication + score_location, data = train)\n",
    "mod32 <- lm(t_response ~ score_accuracy + score_checkin + score_location, data = train)\n",
    "mod33 <- lm(t_response ~ score_accuracy + score_cleanliness + score_location, data = train)\n",
    "mod34 <- lm(t_response ~ score_cleanliness + score_checkin + score_location, data = train)\n",
    "mod35 <- lm(t_response ~ score_cleanliness + score_communication + score_location, data = train)\n",
    "mod41 <- lm(t_response ~ score_cleanliness + score_accuracy + score_checkin + score_location, data = train)\n",
    "mod42 <- lm(t_response ~ score_cleanliness + score_accuracy + score_communication + score_location, data = train)\n",
    "mod43 <- lm(t_response ~ score_checkin + score_accuracy + score_communication + score_location, data = train)\n",
    "mod44 <- lm(t_response ~ score_cleanliness + score_checkin + score_communication + score_location, data = train)\n",
    "mod5 <- lm(t_response ~ score_cleanliness + score_accuracy + score_checkin + score_communication + score_location, data = train)\n",
    "\n",
    "summary(mod1)\n",
    "\n",
    "## all possible models\n",
    "vif(mod21)\n",
    "vif(mod22)\n",
    "vif(mod23)\n",
    "vif(mod24)\n",
    "vif(mod31)\n",
    "vif(mod32)\n",
    "vif(mod33)\n",
    "vif(mod34)\n",
    "vif(mod35)\n",
    "vif(mod41)\n",
    "vif(mod42)\n",
    "vif(mod43)\n",
    "vif(mod44)\n",
    "vif(mod5)\n",
    "\n",
    "summary(mod33)\n",
    "summary(mod34)\n",
    "summary(mod41)\n",
    "summary(mod42)\n",
    "\n",
    "mod24test <- lm(t_response ~ score_cleanliness + score_location, data = test_data)\n",
    "mod34test <- lm(t_response ~ score_cleanliness + score_checkin + score_location, data = test_data)\n",
    "mod35test <- lm(t_response ~ score_cleanliness + score_communication + score_location, data = test_data)\n",
    "mod41test <- lm(t_response ~ score_cleanliness + score_accuracy + score_checkin + score_location, data = test_data)\n",
    "mod42test <- lm(t_response ~ score_cleanliness + score_accuracy + score_communication + score_location, data = test_data)\n",
    "\n",
    "\n",
    "summary(mod24test)\n",
    "summary(mod34test)\n",
    "summary(mod35test)\n",
    "summary(mod41test)\n",
    "summary(mod42test)\n",
    "\n",
    "\n",
    "# low VIF\n",
    "vif(mod33) # 0.1592\n",
    "vif(mod34) # 0.1591\n",
    "vif(mod41) # 0.1742\n",
    "vif(mod42) # 0.1737\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff710d6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Figure 3: Performance of Selected Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1e042e",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "mod <- c(\"1\", \"2\", \"3\", \"4\")\n",
    "predictor1_VIF <- c(\"1.128\", \"1.098\", \"1.131\", \"1.128\")\n",
    "predictor2_VIF <- c(\"2.665\", \"1.510\", \"2.667\", \"2.668\")\n",
    "predictor3_VIF <- c(\"2.824\", \"x\", \"4.095\", \"4.052\")\n",
    "predictor4_VIF <- c(\"x\", \"x\", \"x\", \"2.325\")\n",
    "predictor5_VIF <- c(\"x\", \"1.541\", \"2.235\", \"x\")\n",
    "adjusted_r2 <- c(\"0.164\", \"0.159\", \"0.174\", \"0.174\")\n",
    "AIC <- c(\"306.9\", \"309.2\", \"302.5\", \"302.7\")\n",
    "cAIC <- c(\"307.0\", \"309.3\", \"302.7\", \"302.9\")\n",
    "BIC <- c(\"327.3\", \"329.6\", \"326.8\", \"327.1\")\n",
    "\n",
    "cols <- data.frame(mod, predictor1_VIF, predictor2_VIF, predictor3_VIF, predictor4_VIF, predictor5_VIF, adjusted_r2, AIC, cAIC, BIC)\n",
    "\n",
    "knitr::kable(cols, \"pipe\", col.names = c(\"Model\", \"Location VIF\", \"Cleanliness VIF\", \"Accuracy VIF\", \"Comm. VIF\", \"Checkin VIF\", \"Adjusted R.Sq\", \"AIC\", \"Corrected AIC\", \"BIC\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1cecf9",
   "metadata": {},
   "source": [
    "We used the all possible subsets methods and other auto selection methods to produce 21 unique possible models. Due to the nature of our research question as well as very low adjusted $R^2$, we eliminated all the 1 and 2 predictor models. VIF values were a lot higher in the full model compared to others, so the full model was eliminated.\n",
    "\n",
    "We selected the top 4 models measured by degrees of collinearity, conditions & assumptions, and goodness of fit. In general, all of them had relatively low VIF's, good model assumptions, and high $R^2$ compared to the rest.\n",
    "\n",
    "### 6. Final Selection\n",
    "\n",
    "In the end, we decided to go with model 3. We believe it was the best model because it had relatively low VIF values besides one of them being $> 4$, good model assumptions, relatively high adjusted $R^2$, and the lowest AIC, corrected AIC, and BIC values.\n",
    "\n",
    "Final model: $log(price) = \\beta_0 + \\beta_1Location + \\beta_2Cleanliness + \\beta_3Accuracy  + \\beta_4Checkin + \\epsilon$\n",
    "\n",
    "### 7. Conditions & Model Assumptions of Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de700f35",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "remove_cell",
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# model assumptions mod41 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "mod41 <- lm(t_response ~ score_cleanliness + score_accuracy + score_checkin + score_location, data = train)\n",
    "r <- rstandard(mod41)\n",
    "qqnorm(r)\n",
    "qqline(r)\n",
    "# condition 2\n",
    "pairs(train_data[c(1,2,3,5)], lower.panel = NULL)\n",
    "# condition 1\n",
    "plot(log(train$price) ~ fitted(mod41), main=\"Y versus Y-hat\", xlab=\"Y-hat\", ylab=\"Y\")\n",
    "abline(a = 0, b = 1)\n",
    "lines(lowess(log(train$price) ~ fitted(mod41)), lty=2)\n",
    "# residual plots\n",
    "par(mfrow=c(2,2))\n",
    "plot(r ~ fitted(mod41))\n",
    "plot(r ~ train$score_accuracy, main=\"title\", xlab=\"accuracy\", ylab=\"res\")\n",
    "plot(r ~ train$score_location, main=\"title\", xlab=\"location\", ylab=\"res\")\n",
    "plot(r ~ train$score_cleanliness, main=\"title\", xlab=\"location\", ylab=\"res\")\n",
    "plot(r ~ train$score_checkin, main=\"title\", xlab=\"location\", ylab=\"res\")\n",
    "\n",
    "summary(mod41)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef0671d",
   "metadata": {},
   "source": [
    "Condition 1 and 2 are satisfied and residual plots look good. Hence, linearity, uncorrelated errors, and constant variance assumptions are satisfied. There are some lifts of points on both ends of the QQ plot, but normality is still reasonably satisfied. Overall, conditions and assumptions are satisfied. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1844aa5c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### 8. Model Interpretation & Importance\n",
    "\n",
    "### Figure 4: Summary Statistics in Training and Test Dataset, each of size 430\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d54c50",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "coefficients <- c(\"(Intercept)\", \"Cleanliness\", \"Accuracy\", \"Checkin\", \"Location\")\n",
    "estimate <- c(\"0.758\", \"0.188\", \"-0.727\", \"0.496\", \"0.910\")\n",
    "sd <- c(\"0.710\", \"0.120\", \"0.246\", \"0.195\", \"0.091\")\n",
    "p <- c(\"0.286\", \"0.118\", \"0.00324*\", \"0.01135*\", \"2e-16*\")\n",
    "\n",
    "cols <- data.frame(coefficients, estimate, sd, p)\n",
    "\n",
    "knitr::kable(cols, \"pipe\", col.names = c(\"Coefficients\", \"Estimate\", \"Std. Error\", \"Pr(>|t|)\"), \n",
    "             caption = \"Summary of Model (Training)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e831322",
   "metadata": {},
   "source": [
    "We see that the T-test for variable, cleanliness, is not significant, which means how clean a room is might not have significant impact on its price. When all other variables are held constant, when accuracy score increases by 1, $log(price)$ decreases by 0.727. This indicates how accurate a listing is described with up-to-date photos and info has a negative impact on price. One possible explanation is when photos and information of a listing are \"enhanced\" or described better than it actually is, hence worse accuracy, Airbnb hosts tend to be able to charge more. When checkin score increases by 1, $log(price)$ increases by 0.496. This means hosts who provide better check-in experience for guests are able to charge more for their rooms. When location score is increased by 1, the $log(price)$ is increased by 0.810. This means when a listing is well-located, hosts are able to charge more. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1fdd6d",
   "metadata": {},
   "source": [
    "### 9. Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6f9922",
   "metadata": {
    "tags": [
     "remove_cell",
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# model assumptions mod41_test!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "mod41_test <- lm(t_response ~ score_cleanliness + score_accuracy + score_checkin + score_location, data = test_data)\n",
    "r <- rstandard(mod41_test)\n",
    "qqnorm(r)\n",
    "qqline(r)\n",
    "# condition 2\n",
    "pairs(test_data[c(1,2,3,5)], lower.panel = NULL)\n",
    "# condition 1\n",
    "plot(test_data$t_response ~ fitted(mod41_test), main=\"Y versus Y-hat\", xlab=\"Y-hat\", ylab=\"Y\")\n",
    "abline(a = 0, b = 1)\n",
    "lines(lowess(test_data$t_response ~ fitted(mod41_test)), lty=2)\n",
    "# residual plots\n",
    "par(mfrow=c(2,2))\n",
    "plot(r ~ fitted(mod41_test))\n",
    "plot(r ~ test_data$score_accuracy, main=\"title\", xlab=\"accuracy\", ylab=\"res\")\n",
    "plot(r ~ test_data$score_location, main=\"title\", xlab=\"location\", ylab=\"res\")\n",
    "plot(r ~ test_data$score_cleanliness, main=\"title\", xlab=\"location\", ylab=\"res\")\n",
    "plot(r ~ test_data$score_checkin, main=\"title\", xlab=\"location\", ylab=\"res\")\n",
    "\n",
    "vif(mod41_test)\n",
    "AIC(mod41_test)\n",
    "BIC(mod41_test)\n",
    "summary(mod41)\n",
    "summary(mod41_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485c6404",
   "metadata": {},
   "source": [
    "### Figure 5: Model Comparisons: Training vs. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7508f4f3",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Variable <- c(\"log(Price)\", \"Location\", \"Cleanliness\", \"Accuracy\", \"Communication\", \"Checkin\")\n",
    "mean_sd_train <- c(\"4.43(0.38)\", \"4.77(0.19)\", \"4.76(0.22)\", \"4.82(0.14)\", \"4.87(0.14)\", \"4.87(0.13)\")\n",
    "mean_sd_test <- c(\"4.42(0.39)\", \"4.76(0.20)\", \"4.79(0.21)\", \"4.83(0.13)\", \"4.87(0.13)\", \"4.88(0.12)\")\n",
    "\n",
    "cols <- data.frame(Variable, mean_sd_train, mean_sd_test)\n",
    "\n",
    "knitr::kable(cols, \"pipe\", col.names = c(\"Variable\", \"Mean(s.d) in Training\", \"Mean(s.d) in Test\"), caption = \"Summary Statistics in Training and Test Datasets, each of size 430\")\n",
    "\n",
    "coefficients <- c(\"(Intercept)\", \"Cleanliness\", \"Accuracy\", \"Checkin\", \"Location\")\n",
    "estimate <- c(\"0.511\", \"0.450\", \"-0.895\", \"0.328\", \"0.937\")\n",
    "sd <- c(\"0.732\", \"0.127\", \"0.247\", \"0.214\", \"0.091\")\n",
    "p <- c(\"0.486\", \"0.000414*\", \"0.000330*\", \"0.126\", \"2e-16*\")\n",
    "\n",
    "cols <- data.frame(coefficients, estimate, sd, p)\n",
    "\n",
    "knitr::kable(cols, \"pipe\", col.names = c(\"Coefficients\", \"Estimate\", \"Std. Error\", \"Pr(>|t|)\"), \n",
    "             caption = \"Summary of Model (Test)\")\n",
    "\n",
    "mod <- c(\"Training\", \"Test\")\n",
    "predictor1_VIF <- c(\"1.131\", \"1.163\")\n",
    "predictor2_VIF <- c(\"2.667\", \"2.472\")\n",
    "predictor3_VIF <- c(\"4.095\", \"3.809\")\n",
    "predictor4_VIF <- c(\"x\", \"x\")\n",
    "predictor5_VIF <- c(\"2.235\", \"2.386\")\n",
    "adjusted_r2 <- c(\"0.174\", \"0.237\")\n",
    "AIC <- c( \"302.5\", \"309.8\")\n",
    "cAIC <- c(\"302.7\", \"310.3\")\n",
    "BIC <- c(\"326.8\", \"334.2\")\n",
    "RSS <- c(\"0.341\", \"0.344\")\n",
    "\n",
    "cols <- data.frame(mod, predictor1_VIF, predictor2_VIF, predictor3_VIF, predictor4_VIF, predictor5_VIF, adjusted_r2, AIC, cAIC, BIC, RSS)\n",
    "\n",
    "par(mfrow=c(1,2))\n",
    "\n",
    "knitr::kable(cols, \"pipe\", col.names = c(\"Model\", \"Location VIF\", \"Cleanliness VIF\", \"Accuracy VIF\", \"Comm. VIF\", \"Checkin VIF\", \"Adjusted R.sq\", \"AIC\", \"Corrected AIC\", \"BIC\", \"RSS\"), \n",
    "             caption = \"Model Properties: Test vs. Training\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a89dd58",
   "metadata": {},
   "source": [
    "Summary statistics in both datasets appear similar. Conditions and assumptions of the model for both training and test dataset are similar. There are more influential points in the test dataset than training: 66 vs. 46 respectively. RSS is similar at around 0.34. Adjusted R-squared are a bit different: 0.174 and 0.237 for training and test dataset respectively.\n",
    "\n",
    "The predictor, cleanliness, is insignificant in the training dataset but significant in test dataset. On the other hand, checkin is significant in the training dataset but insignificant in the training dataset. The estimated coefficients of accuracy, location, checkin score, and the intercept are all within 2 standard deviations compared to the ones in test dataset. Due to the significant differences in T-tests and adjusted $R^2$, our final model is not validated.\n",
    "\n",
    "\\newpage\n",
    "\n",
    "## Limitations\n",
    "\n",
    "It is important to note that our results should be taken with a grain of salt because we had many influential points in our training dataset, normality assumption is not perfectly satisfied, and most importantly, our final model is not validated. Other transformations may have improved normality assumption but were ultimately not used because they jeopardized other conditions and assumptions. Our inability to validate the final model may be due to the differences in observations and large number of influential points in both datasets.\n",
    "\n",
    "The large number of influential points and non-perfect normality assumption can introduce biases in our results, which means are the our T-tests for each estimated coefficients as well as the estimated coefficients themselves may not accurate and reliable. \n",
    "\n",
    "## References\n",
    "\n",
    "1. Jorge Chica-Olmo, Juan Gabriel GonzÃ¡lez-Morales, JosÃ© Luis Zafra-GÃ³mez,\n",
    "Effects of location on Airbnb apartment pricing in MÃ¡laga,\n",
    "Tourism Management,\n",
    "Volume 77,\n",
    "2020,\n",
    "103981,\n",
    "ISSN 0261-5177,\n",
    "https://doi.org/10.1016/j.tourman.2019.103981.\n",
    "\n",
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25608ce3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Figure 1: Predictor vs. Predictor Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9c40d4",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# condition 2\n",
    "\n",
    "pairs(clean1[6:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22678df",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Figure 2: Influential Points in Predictor vs. Response Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67684334",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "par(mfrow=c(2,3))\n",
    "plot(train$t_response~train$score_cleanliness, main=\"Response vs Cleanliness\", xlab=\"Cleanliness\", ylab=\"log(Price)\")\n",
    "points(train$t_response[w] ~train$score_cleanliness[w], col=\"orange\", pch=19)\n",
    "plot(train$t_response~train$score_accuracy, main=\"Response vs accuracy\", xlab=\"accuracy\", ylab=\"log(Price)\")\n",
    "points(train$t_response[w] ~train$score_accuracy[w], col=\"orange\", pch=19)\n",
    "plot(train$t_response~train$score_location, main=\"Response vs location\", xlab=\"location\", ylab=\"log(Price)\")\n",
    "points(train$t_response[w] ~train$score_location[w], col=\"orange\", pch=19)\n",
    "plot(train$t_response~train$score_communication, main=\"Response vs communication\", xlab=\"communication\", ylab=\"log(Price)\")\n",
    "points(train$t_response[w] ~train$score_communication[w], col=\"orange\", pch=19)\n",
    "plot(train$t_response~train$score_checkin, main=\"Response vs checkin\", xlab=\"checkin\", ylab=\"log(Price)\")\n",
    "points(train$t_response[w] ~train$score_checkin[w], col=\"orange\", pch=19)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6992240f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71290710",
   "metadata": {},
   "source": [
    "### word count: 1695\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "fig.height,name,fig.width,tags,-all",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".Rmd",
    "format_name": "rmarkdown"
   }
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
